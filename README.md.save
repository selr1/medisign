# SEtup

```

# Clone the repository
git clone https://github.com/selr1/medisign.git

# Create new conda environment with Python 3.10
conda create -n MediSign_MSL python=3.10 -y

# Activate environment in Fish shell
conda activate MediSign_MSL

# Install all required libraries
pip install -r requirements.txt

```
# Folder structure
- To replicate our current setup, the folder structure must look like this :

```
MediSign/
├── CSVs/                 # Folder for extracted landmark data
├── Datasets/             # Source images (Words, V1, V2)
├── Scripts/              # All python scripts and saved models
└── requirements.txt      # Project dependencies

```
# Requirements status

#### a) Datasets Selection & Expansion
- We used 2 datasets [MyWSL2023](https://data.mendeley.com/datasets/zvk55p7ktd/1) from Mendeley 
and [Malaysian Sign Language (MSL) Image Dataset](https://www.kaggle.com/datasets/pradeepisawasan/malaysian-sign-language-msl-image-dataset) from Kaggle.
- [Preprocessing script](Scripts/prep.py) consists of crawlers that look for image files, take the parent folder name and use it as the ground truth.
#### b) Algorithm Implementation
-
# NOtes
#### Training the AI with 2 datasets that displays different type of handsigns was proven to be confusing. 

- Our first dataset only shows standard handsigns meaning the extracted data was only for landmark of hands.
- The second dataset however was a broader type of data meaning it involves interaction of hands with different bodypart such as mouth and stomach.
- Training the AI on a merged dataset extracted from these 2 causes the AI to confuse between the landmark of hands and its interactions witb other bodyparts.

### Solution : switched to another dataset

- Contain cropped images of single word handsigns related to medical problems.

